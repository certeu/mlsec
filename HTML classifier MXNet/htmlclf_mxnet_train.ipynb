{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1330,
     "status": "ok",
     "timestamp": 1604569412545,
     "user": {
      "displayName": "Bernd Schomburg",
      "photoUrl": "",
      "userId": "04690499344915037675"
     },
     "user_tz": -60
    },
    "id": "6wfEw3CEGYbh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "\n",
    "from extractor import extract_features\n",
    "from model_mxnet import custom_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1142,
     "status": "ok",
     "timestamp": 1604569289850,
     "user": {
      "displayName": "Bernd Schomburg",
      "photoUrl": "",
      "userId": "04690499344915037675"
     },
     "user_tz": -60
    },
    "id": "jux-01-gGYbk"
   },
   "outputs": [],
   "source": [
    "# Custom dataset to load the data\n",
    "class CustomDataset(gluon.data.Dataset):\n",
    "    def __init__(self, path_to_b_files, path_to_m_files, features_size=1024):\n",
    "        self.features_size = features_size\n",
    "        b_files = [os.path.join(path_to_b_files, f) for f in os.listdir(path_to_b_files)]\n",
    "        m_files = [os.path.join(path_to_m_files, f) for f in os.listdir(path_to_m_files)]\n",
    "        self.list_files = b_files + m_files\n",
    "        self.length = len(self.list_files)\n",
    "        self.labels = mx.nd.concat(mx.nd.zeros(shape=(len(b_files))),\n",
    "                                   mx.nd.ones(shape=(len(m_files))),\n",
    "                                   dim=0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.list_files[idx], 'rb') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        data = extract_features(content, hash_dim=self.features_size, split_regex=rb\"\\s+\")\n",
    "        return mx.nd.array(data), self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1604569297294,
     "user": {
      "displayName": "Bernd Schomburg",
      "photoUrl": "",
      "userId": "04690499344915037675"
     },
     "user_tz": -60
    },
    "id": "1N9YfgWYGYbn"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "LOG_INTERVAL = 100\n",
    "VAL_INTERVAL = 1\n",
    "FEATURES_SIZE = 1024\n",
    "\n",
    "# Fixed the seed for randomness\n",
    "mx.random.seed(999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 501,
     "status": "ok",
     "timestamp": 1604569300555,
     "user": {
      "displayName": "Bernd Schomburg",
      "photoUrl": "",
      "userId": "04690499344915037675"
     },
     "user_tz": -60
    },
    "id": "FfHltABYGYbq"
   },
   "outputs": [],
   "source": [
    "# Function to get train and val dataloader\n",
    "def get_dataloader():\n",
    "    path_to_train_b_files = 'data/html/benign_files/training/'\n",
    "    path_to_train_m_files = 'data/html/malicious_files/training/'\n",
    "    path_to_validation_b_files = 'data/html/benign_files/validation/'\n",
    "    path_to_validation_m_files = 'data/html/malicious_files/validation/'\n",
    "\n",
    "    train_dataset = CustomDataset(path_to_train_b_files,\n",
    "                                  path_to_train_m_files,\n",
    "                                  FEATURES_SIZE)\n",
    "    train_dataloader = mx.gluon.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                                                num_workers=8, shuffle=True)\n",
    "\n",
    "    val_dataset = CustomDataset(path_to_validation_b_files,\n",
    "                                path_to_validation_m_files,\n",
    "                                FEATURES_SIZE)\n",
    "    val_dataloader = mx.gluon.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                                              num_workers=8, shuffle=False)\n",
    "\n",
    "    return train_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1573,
     "status": "ok",
     "timestamp": 1604569305324,
     "user": {
      "displayName": "Bernd Schomburg",
      "photoUrl": "",
      "userId": "04690499344915037675"
     },
     "user_tz": -60
    },
    "id": "CRXO2UAiGYbx"
   },
   "outputs": [],
   "source": [
    "# Function to get binary labels\n",
    "def facc(label, pred):\n",
    "    pred = pred.ravel()\n",
    "    label = label.ravel()\n",
    "    return ((pred > 0.5) == label).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1194,
     "status": "ok",
     "timestamp": 1604569307800,
     "user": {
      "displayName": "Bernd Schomburg",
      "photoUrl": "",
      "userId": "04690499344915037675"
     },
     "user_tz": -60
    },
    "id": "Vca0kQ90GYb0"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate accuracy for a model\n",
    "def evaluate(model, val_data, ctx):\n",
    "    metric = mx.metric.CustomMetric(facc)\n",
    "    for data, label in val_data:\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = model(data)\n",
    "        metric.update(label, output)\n",
    "\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1081,
     "status": "ok",
     "timestamp": 1604569309840,
     "user": {
      "displayName": "Bernd Schomburg",
      "photoUrl": "",
      "userId": "04690499344915037675"
     },
     "user_tz": -60
    },
    "id": "4s4Aa_0KGYb3",
    "outputId": "53a6cf2f-242c-43c6-a375-1998839a6359"
   },
   "outputs": [],
   "source": [
    "if mx.context.num_gpus() > 0:\n",
    "    print(\"Running the script on single GPU\")\n",
    "    ctx = mx.gpu(0)\n",
    "else:\n",
    "    print(\"Running the script on CPU\")\n",
    "    ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 398,
     "status": "error",
     "timestamp": 1604569420070,
     "user": {
      "displayName": "Bernd Schomburg",
      "photoUrl": "",
      "userId": "04690499344915037675"
     },
     "user_tz": -60
    },
    "id": "l_kTeNhAGYb5",
    "outputId": "9a002b3f-a655-48fc-f502-e6d09e69d411"
   },
   "outputs": [],
   "source": [
    "# Create a model\n",
    "net = custom_model()\n",
    "net.cast('float32')\n",
    "net.hybridize(static_alloc=True, static_shape=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCPBvqFvGYb8"
   },
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "initializer = mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\",\n",
    "                             magnitude=2)\n",
    "net.initialize(initializer, ctx=ctx)\n",
    "\n",
    "# Create optimizer\n",
    "optimizer_params = {'learning_rate': 0.02, 'momentum': 0.9}\n",
    "\n",
    "opt = mx.optimizer.create('sgd', **optimizer_params)\n",
    "trainer = gluon.Trainer(net.collect_params(), opt)\n",
    "loss_fn = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O396agq5GYb_"
   },
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxCPrxPKGYcC"
   },
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train(net, train_dataloader, val_dataloader):\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    train_metric = mx.metric.CustomMetric(facc)\n",
    "    start = time.time() #B\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print('Epoch {}/{}'.format(epoch+1, EPOCHS))\n",
    "        print('-' * 10)\n",
    "        tic = time.time()\n",
    "        # reset metric at beginning of epoch.\n",
    "        train_metric.reset()\n",
    "        for i, (data, label) in enumerate(train_dataloader):\n",
    "            # Copy data to ctx if necessary\n",
    "            data = data.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # Start recording computation graph with record() section.\n",
    "            # Recorded graphs can then be differentiated with backward.\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                L = loss_fn(output, label)\n",
    "            L.backward()\n",
    "            curr_loss = mx.ndarray.mean(L).asscalar()\n",
    "\n",
    "            # take a gradient step with batch_size equal to data.shape[0]\n",
    "            trainer.step(BATCH_SIZE)\n",
    "            # update metric at last.\n",
    "            train_metric.update(label, output)\n",
    "\n",
    "            if i % LOG_INTERVAL == 0:\n",
    "                name, acc = train_metric.get()\n",
    "                print('[Epoch %d Batch %4d] Training loss: %3.4f accuracy: %2.4f' %\n",
    "                             (epoch +1, i, curr_loss, acc))\n",
    "        elapsed = time.time() - tic\n",
    "        speed = i * BATCH_SIZE / elapsed\n",
    "        print('Epoch %d Speed=%.2f samples/sec Time cost=%f secs'% (epoch+1, speed, elapsed))\n",
    "        \n",
    "        # Evaluate the model\n",
    "        if (epoch + 1) % VAL_INTERVAL == 0:\n",
    "            val_name, val_acc = evaluate(net, val_dataloader, ctx)\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                net.save_parameters('net.params')\n",
    "            print('Validation accuracy: %f' % (val_acc))\n",
    "            print()\n",
    "    print()        \n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best validation accuracy: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    net.load_parameters('net.params', ctx = ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 203079,
     "status": "ok",
     "timestamp": 1604511117715,
     "user": {
      "displayName": "Bernd Schomburg",
      "photoUrl": "",
      "userId": "04690499344915037675"
     },
     "user_tz": -60
    },
    "id": "p0JCHvgZGYcE",
    "outputId": "037c8215-8113-4c0c-ff04-a6fdbb8721a8"
   },
   "outputs": [],
   "source": [
    "train(net, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "htmlclf_mxnet_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
